# Evidence Generation via Web Search and Scraping

This document describes the autogenerated evidence feature for competitors.

## Overview

Users can now automatically generate structured evidence drafts for competitors by:
1. Searching for a competitor by name or pasting a URL
2. Selecting the correct domain/website
3. Generating an evidence draft from scraped web content
4. Reviewing and editing the draft before saving

## Architecture

### Components

1. **Search Provider** (`lib/search/`)
   - Interface-based design supporting multiple providers (Tavily, SerpAPI)
   - Automatically selects provider based on available API keys
   - API route: `/api/search` (POST { q: string })

2. **URL Extraction** (`lib/extract/`)
   - `targets.ts`: Builds list of target URLs (homepage, pricing, features, etc.)
   - `fetchAndExtract.ts`: Fetches HTML and extracts readable text
   - Max 5 pages per competitor
   - Max 12,000 characters per page (truncated if needed)

3. **Evidence Generation** (`app/api/evidence/generate/route.ts`)
   - API route that orchestrates search → scrape → LLM generation flow
   - Caches extracted sources for 24 hours
   - Returns structured evidence draft with citations
   - Uses Zod validation for request body
   - Returns `{ ok: true, draft }` or `{ ok: false, error }` response format

4. **UI Components**
   - `EvidenceGenerator.tsx`: Search interface, results picker, generation button
   - Integrated into `CompetitorForm.tsx`
   - Shows progress updates during generation

### Database Schema

#### `evidence_sources` Table
Stores scraped web content with RLS protection:
- `id`: UUID primary key
- `project_id`: References projects (cascade delete)
- `competitor_id`: Optional reference to competitors (null on creation, can be linked later)
- `domain`: Domain name for caching/grouping
- `url`: Full URL of scraped page
- `extracted_text`: Cleaned text content (max 12k chars)
- `page_title`: Extracted page title
- `extracted_at`: Timestamp for cache TTL
- `created_at`: Record creation timestamp

#### `competitors` Table
Added optional field:
- `evidence_citations`: JSONB field for storing citation metadata (optional)

## Usage Flow

1. User types competitor name or URL in search box
2. System searches web (via Tavily/SerpAPI) and shows results
3. User selects correct domain/website
4. User clicks "Generate evidence"
5. System:
   - Checks cache (24h TTL per domain+project)
   - If cache miss, fetches up to 5 target pages
   - Extracts text from each page
   - Stores sources in `evidence_sources`
   - Calls LLM to generate structured draft
6. Draft is formatted with citations and inserted into evidence textarea
7. User can edit draft before saving competitor

## Configuration

### Environment Variables

- `TAVILY_API_KEY`: Primary search provider (preferred)
- `SERPAPI_API_KEY`: Fallback search provider
- At least one must be set

### Constants (`lib/constants.ts`)

- `MAX_PAGES_PER_COMPETITOR = 5`: Maximum pages to scrape per competitor
- `MAX_EXTRACTED_CHARS_PER_PAGE = 12_000`: Maximum characters per page (truncation limit)
- `EVIDENCE_CACHE_TTL_HOURS = 24`: Cache TTL for extracted sources

## Evidence Draft Schema

The LLM generates a structured JSON draft:

```typescript
{
  competitor_name: string
  domain: string
  sections: {
    positioning: { bullets: string[], sources: string[] }
    pricing: { bullets: string[], sources: string[] }
    target_customers: { bullets: string[], sources: string[] }
    key_features: { bullets: string[], sources: string[] }
    integrations?: { bullets: string[], sources: string[] } // Optional
    enterprise_signals?: { bullets: string[], sources: string[] } // Optional
  }
}
```

Each section must have at least one source URL. The draft is formatted as markdown with citations when applied to the form.

## Security & Privacy

- Only public pages are scraped (no authentication)
- RLS policies ensure users can only access sources for their own projects
- Sources are cached per project+domain to reduce costs
- No confidential information should be pasted (user warning displayed)

## Error Handling

- Search failures: User can still enter manually
- Scraping failures: Individual page failures don't block generation (continue with available pages)
- LLM failures: Clear error message, user can try again or enter manually
- Network timeouts: 15-second timeout per page fetch

## Future Enhancements

- Link evidence sources to competitors after creation (update `competitor_id`)
- Support for custom target URLs
- Better handling of JavaScript-rendered pages (currently only static HTML)
- Export/import of evidence sources
- Manual source management UI

